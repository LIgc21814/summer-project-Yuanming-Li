{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84f38824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from os import path\n",
    "import pandas as pd\n",
    "import pdb\n",
    "import logging\n",
    "import re\n",
    "from gensim import parsing\n",
    "import gensim\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1ab57ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file = path.join('semeval2016-task6-trainingdata.txt')\n",
    "training_data = pd.read_csv(training_file, header=0, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4293d2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_file = path.join('SemEval2016-Task6-subtaskA-testdata-gold.txt')\n",
    "testing_data = pd.read_csv(testing_file,header=0, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4014d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    #convert text to lower case\n",
    "    text = text.lower()\n",
    "   \n",
    "    #removing whitespace\n",
    "    text.strip()\n",
    "   \n",
    "    #removing digits\n",
    "    text = gensim.parsing.preprocessing.strip_numeric(text)\n",
    "    #text = ' '.join(s for s in text.split() if not any(c.isdigit() for c in s))\n",
    "    \n",
    "    #remove stopwords\n",
    "    text = gensim.parsing.preprocessing.remove_stopwords(text)\n",
    "    \n",
    "    #strip punctutation\n",
    "    text = gensim.parsing.preprocessing.strip_punctuation(text)\n",
    "    \n",
    "    #strip multiple whitepsace that might occur after we remove stopwords\n",
    "    text = gensim.parsing.preprocessing.strip_multiple_whitespaces(text)\n",
    "\n",
    "    p = PorterStemmer()\n",
    "    \n",
    "    text = ' '.join(p.stem(word) for word in text.split())    \n",
    "\n",
    "    #print(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cd40751",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['Tweet'] = training_data['Tweet'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc1f3563",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data['Tweet'] = testing_data['Tweet'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c36ddccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Target</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>exalt shall humbl humbl shall exalt matt semst</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10002</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>rt prayerbullet remov nehushtan previou move g...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10003</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>brainman heidtjj benjaminl sought truth soul s...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10004</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>god utterli powerless human intervent semst</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10005</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>david cameron miracl multicultur miracl shadi ...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>11245</td>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>metalheadmonti tom six follow watch human cent...</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>11246</td>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>aveng blood rememb ignor afflict ps comequickl...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>11247</td>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>life sacr level abort comput philosophi kate m...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>11248</td>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>ravensymon u refer we you minor idiot support ...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>11249</td>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>al robertson s mom duckdynasti chose life unw ...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1249 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                    Target  \\\n",
       "0     10001                   Atheism   \n",
       "1     10002                   Atheism   \n",
       "2     10003                   Atheism   \n",
       "3     10004                   Atheism   \n",
       "4     10005                   Atheism   \n",
       "...     ...                       ...   \n",
       "1244  11245  Legalization of Abortion   \n",
       "1245  11246  Legalization of Abortion   \n",
       "1246  11247  Legalization of Abortion   \n",
       "1247  11248  Legalization of Abortion   \n",
       "1248  11249  Legalization of Abortion   \n",
       "\n",
       "                                                  Tweet   Stance  \n",
       "0        exalt shall humbl humbl shall exalt matt semst  AGAINST  \n",
       "1     rt prayerbullet remov nehushtan previou move g...  AGAINST  \n",
       "2     brainman heidtjj benjaminl sought truth soul s...  AGAINST  \n",
       "3           god utterli powerless human intervent semst  AGAINST  \n",
       "4     david cameron miracl multicultur miracl shadi ...  AGAINST  \n",
       "...                                                 ...      ...  \n",
       "1244  metalheadmonti tom six follow watch human cent...     NONE  \n",
       "1245  aveng blood rememb ignor afflict ps comequickl...  AGAINST  \n",
       "1246  life sacr level abort comput philosophi kate m...  AGAINST  \n",
       "1247  ravensymon u refer we you minor idiot support ...  AGAINST  \n",
       "1248  al robertson s mom duckdynasti chose life unw ...  AGAINST  \n",
       "\n",
       "[1249 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "477c3c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Target</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>dear lord thank u ur bless forgiv sin lord str...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>bless peacemak shall call children god matthew...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>conform world transform renew mind ispeaklif g...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>salah prai focu understand allah warn lazi pra...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>stai hous displai like time ignor quran islam ...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2809</th>\n",
       "      <td>2910</td>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>there s law protect unborn eagl human uh idk y...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2810</th>\n",
       "      <td>2911</td>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>abort abortionondemand menstruationmatt semst</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2811</th>\n",
       "      <td>2912</td>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>dare sexual prefer choic dare dismemb preborn ...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2812</th>\n",
       "      <td>2913</td>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>equal right born wai right born liberallog lib...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2813</th>\n",
       "      <td>2914</td>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>potu seal legaci w doz win gop agenda still re...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2814 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID                    Target  \\\n",
       "0      101                   Atheism   \n",
       "1      102                   Atheism   \n",
       "2      103                   Atheism   \n",
       "3      104                   Atheism   \n",
       "4      105                   Atheism   \n",
       "...    ...                       ...   \n",
       "2809  2910  Legalization of Abortion   \n",
       "2810  2911  Legalization of Abortion   \n",
       "2811  2912  Legalization of Abortion   \n",
       "2812  2913  Legalization of Abortion   \n",
       "2813  2914  Legalization of Abortion   \n",
       "\n",
       "                                                  Tweet   Stance  \n",
       "0     dear lord thank u ur bless forgiv sin lord str...  AGAINST  \n",
       "1     bless peacemak shall call children god matthew...  AGAINST  \n",
       "2     conform world transform renew mind ispeaklif g...  AGAINST  \n",
       "3     salah prai focu understand allah warn lazi pra...  AGAINST  \n",
       "4     stai hous displai like time ignor quran islam ...  AGAINST  \n",
       "...                                                 ...      ...  \n",
       "2809  there s law protect unborn eagl human uh idk y...  AGAINST  \n",
       "2810      abort abortionondemand menstruationmatt semst  AGAINST  \n",
       "2811  dare sexual prefer choic dare dismemb preborn ...  AGAINST  \n",
       "2812  equal right born wai right born liberallog lib...  AGAINST  \n",
       "2813  potu seal legaci w doz win gop agenda still re...  AGAINST  \n",
       "\n",
       "[2814 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caa6d12",
   "metadata": {},
   "source": [
    "# Internal decomposition of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b424f18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb9249f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tweet'] = df.Tweet + ' ' + df.Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e58c1df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dear lord thank u ur bless forgiv sin lord strength energi busi dai ahead bless hope semst Atheism'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Tweet'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06beebba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.3)\n",
    "train0, val = train_test_split(train, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4225ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7184 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 50000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, \n",
    "                      filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', \n",
    "                      split=\" \",\n",
    "                      lower=True)\n",
    "tokenizer.fit_on_texts(df['Tweet'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a9be8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (845, 100)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "X = tokenizer.texts_to_sequences(train0['Tweet'].values)\n",
    "X_train = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "max_len = 0\n",
    "for t in X:\n",
    "    if max_len < len(t):\n",
    "        max_len = len(t)\n",
    "X1 = tokenizer.texts_to_sequences(test['Tweet'].values)\n",
    "for t in X1:\n",
    "    if max_len < len(t):\n",
    "        max_len = len(t)\n",
    "X_test = pad_sequences(X1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Shape of data tensor:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79563860",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = tokenizer.texts_to_sequences(val['Tweet'].values)\n",
    "X_val = pad_sequences(X2, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2762e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (1969, 3)\n"
     ]
    }
   ],
   "source": [
    "Y_train = pd.get_dummies(train['Stance']).values\n",
    "Y_test = pd.get_dummies(test['Stance']).values\n",
    "Y_val = pd.get_dummies(val['Stance']).values\n",
    "print('Shape of label tensor:', Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29e4d854",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = train0['Stance']\n",
    "Y_test = test['Stance']\n",
    "Y_val = val['Stance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f88d311c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4421838",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_train.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48641149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 100)          5000000   \n",
      "                                                                 \n",
      " spatial_dropout1d (SpatialD  (None, 100, 100)         0         \n",
      " ropout1D)                                                       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               80400     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 303       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,080,703\n",
      "Trainable params: 5,080,703\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From C:\\Users\\LYM\\AppData\\Local\\Temp\\ipykernel_13324\\1725990102.py:5: experimental_run_functions_eagerly (from tensorflow.python.eager.def_function) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.run_functions_eagerly` instead of the experimental version.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\LYM\\miniconda3\\envs\\data_analytics_course0\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:264: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Cast string to float is not supported [Op:Cast]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39msummary())\n\u001b[0;32m      5\u001b[0m tf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mexperimental_run_functions_eagerly(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 6\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_delta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Users\\LYM\\miniconda3\\envs\\data_analytics_course0\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mD:\\Users\\LYM\\miniconda3\\envs\\data_analytics_course0\\lib\\site-packages\\keras\\losses.py:1777\u001b[0m, in \u001b[0;36mcategorical_crossentropy\u001b[1;34m(y_true, y_pred, from_logits, label_smoothing, axis)\u001b[0m\n\u001b[0;32m   1751\u001b[0m \u001b[38;5;124;03m\"\"\"Computes the categorical crossentropy loss.\u001b[39;00m\n\u001b[0;32m   1752\u001b[0m \n\u001b[0;32m   1753\u001b[0m \u001b[38;5;124;03mStandalone usage:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;124;03m  Categorical crossentropy loss value.\u001b[39;00m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1776\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(y_pred)\n\u001b[1;32m-> 1777\u001b[0m y_true \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1778\u001b[0m label_smoothing \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(label_smoothing, dtype\u001b[38;5;241m=\u001b[39my_pred\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_smooth_labels\u001b[39m():\n",
      "\u001b[1;31mUnimplementedError\u001b[0m: Cast string to float is not supported [Op:Cast]"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "print(model.summary())\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "history = model.fit(X_train, Y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b6eb0de",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnimplementedError",
     "evalue": "Cast string to float is not supported [Op:Cast]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m accr \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest set\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  Loss: \u001b[39m\u001b[38;5;132;01m{:0.3f}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  Accuracy: \u001b[39m\u001b[38;5;132;01m{:0.3f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(accr[\u001b[38;5;241m0\u001b[39m],accr[\u001b[38;5;241m1\u001b[39m]))\n",
      "File \u001b[1;32mD:\\Users\\LYM\\miniconda3\\envs\\data_analytics_course0\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mD:\\Users\\LYM\\miniconda3\\envs\\data_analytics_course0\\lib\\site-packages\\keras\\losses.py:1777\u001b[0m, in \u001b[0;36mcategorical_crossentropy\u001b[1;34m(y_true, y_pred, from_logits, label_smoothing, axis)\u001b[0m\n\u001b[0;32m   1751\u001b[0m \u001b[38;5;124;03m\"\"\"Computes the categorical crossentropy loss.\u001b[39;00m\n\u001b[0;32m   1752\u001b[0m \n\u001b[0;32m   1753\u001b[0m \u001b[38;5;124;03mStandalone usage:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;124;03m  Categorical crossentropy loss value.\u001b[39;00m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1776\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(y_pred)\n\u001b[1;32m-> 1777\u001b[0m y_true \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1778\u001b[0m label_smoothing \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(label_smoothing, dtype\u001b[38;5;241m=\u001b[39my_pred\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_smooth_labels\u001b[39m():\n",
      "\u001b[1;31mUnimplementedError\u001b[0m: Cast string to float is not supported [Op:Cast]"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11785c83",
   "metadata": {},
   "source": [
    "# Test data using gold tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b576c54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LYM\\AppData\\Local\\Temp\\ipykernel_13324\\3939842013.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  training_data['Tweet'] = training_data['Tweet'].str.replace('\\d+', '')\n",
      "C:\\Users\\LYM\\AppData\\Local\\Temp\\ipykernel_13324\\3939842013.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  testing_data['Tweet'] = testing_data['Tweet'].str.replace('\\d+', '')\n"
     ]
    }
   ],
   "source": [
    "training_data['Tweet'] = training_data.Tweet + ' ' + training_data.Target\n",
    "training_data['Tweet'] = training_data['Tweet'].str.replace('\\d+', '')\n",
    "testing_data['Tweet'] = testing_data.Tweet + ' ' + testing_data.Target\n",
    "testing_data['Tweet'] = testing_data['Tweet'].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cecf019a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7184 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 50000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(training_data['Tweet'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae7786b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (1249, 100)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "train = tokenizer.texts_to_sequences(training_data['Tweet'].values)\n",
    "X_train= pad_sequences(train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test = tokenizer.texts_to_sequences(testing_data['Tweet'].values)\n",
    "X_test = pad_sequences(test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Shape of data tensor:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5c6bbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (1249, 3)\n"
     ]
    }
   ],
   "source": [
    "Y_train = pd.get_dummies(training_data['Stance']).values\n",
    "Y_test = pd.get_dummies(testing_data['Stance']).values\n",
    "print('Shape of label tensor:', Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "762c10e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 100, 100)          5000000   \n",
      "                                                                 \n",
      " spatial_dropout1d_1 (Spatia  (None, 100, 100)         0         \n",
      " lDropout1D)                                                     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128)               117248    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,117,635\n",
      "Trainable params: 5,117,635\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "40/40 [==============================] - 73s 2s/step - loss: 1.0237 - accuracy: 0.5036 - val_loss: 0.9488 - val_accuracy: 0.5993\n",
      "Epoch 2/5\n",
      "40/40 [==============================] - 70s 2s/step - loss: 0.8823 - accuracy: 0.5790 - val_loss: 0.9225 - val_accuracy: 0.6170\n",
      "Epoch 3/5\n",
      "40/40 [==============================] - 72s 2s/step - loss: 0.6945 - accuracy: 0.6872 - val_loss: 0.9798 - val_accuracy: 0.6348\n",
      "Epoch 4/5\n",
      "40/40 [==============================] - 86s 2s/step - loss: 0.5411 - accuracy: 0.7220 - val_loss: 1.2241 - val_accuracy: 0.5745\n",
      "Epoch 5/5\n",
      "40/40 [==============================] - 76s 2s/step - loss: 0.4584 - accuracy: 0.7765 - val_loss: 1.3036 - val_accuracy: 0.5426\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16d5386cd90>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_train.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_train, Y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98df3bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(X, model, num_samples):\n",
    "    preds = [model(X, training=True) for _ in range(num_samples)]\n",
    "    return np.stack(preds).mean(axis=0)\n",
    "     \n",
    "def predict_class(X, model, num_samples):\n",
    "    proba_preds = predict_proba(X, model, num_samples)\n",
    "    return np.argmax(proba_preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0311d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_class(X_test, model, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cd7920",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0579cc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775b7f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = np.mean(y_pred == Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0048e804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f63b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e181df82",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c96c929",
   "metadata": {},
   "outputs": [],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4aa8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f775f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c02b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='test')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10adff",
   "metadata": {},
   "source": [
    "# LSTM+CNN with gold tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbbbe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Flatten, Conv1D, SpatialDropout1D, MaxPooling1D,AveragePooling1D, Bidirectional, concatenate, Input, Dropout, LSTM\n",
    "# from keras.layers import merge\n",
    "\n",
    "\n",
    "y_dim=3\n",
    "num_filters=200\n",
    "filter_sizes=[3,4,5] \n",
    "pool_padding='valid' \n",
    "dropout=0.5\n",
    "\n",
    "embed_input = Input(shape=(X_train.shape[1],))\n",
    "x = Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_train.shape[1])(embed_input)\n",
    "pooled_outputs = []\n",
    "for i in range(len(filter_sizes)):\n",
    "    conv = Conv1D(num_filters, kernel_size=filter_sizes[i], padding='valid', activation='relu')(x)\n",
    "    conv = MaxPooling1D(pool_size=EMBEDDING_DIM-filter_sizes[i]+1)(conv)           \n",
    "    pooled_outputs.append(conv)\n",
    "merge = concatenate(pooled_outputs)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eb898c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "x = Dense(30, activation='relu')(merge)\n",
    "x = Dropout(dropout)(x)\n",
    "x = Bidirectional(LSTM(100, return_sequences=True, dropout=0.5, recurrent_dropout=0.1))(x)\n",
    "x = Dense(30, activation='relu')(x)\n",
    "x = Dropout(dropout)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(y_dim, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=embed_input,outputs=x)\n",
    "model.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "epochs = 3\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3366ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef14219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db54129",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='test')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be164af6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analytics_course0",
   "language": "python",
   "name": "data_analytics_course0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
